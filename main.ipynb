{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pre_processing \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    files = get_text.get_text_data()\n",
    "    weighted_X, ivoc, vocab = weighted_vectorize(files)\n",
    " \n",
    "    perplexity =list()\n",
    "    likelihood = list()\n",
    "    #for tuning \n",
    "    topic_n_range = range(2, 10)\n",
    "    for topic_n in [2, 5, 10, 15, 20, 25, 30]:\n",
    "\n",
    "        lda = LatentDirichletAllocation(evaluate_every=5,verbose=1,learning_decay= 0.7,\n",
    "                                        learning_method='batch',n_components=topic_n)\n",
    "        theta = lda.fit_transform(weighted_X);\n",
    "        print(\"Log Likelihood: \", lda.score(weighted_X))\n",
    "        print(\"Perplexity: \", lda.perplexity(weighted_X))\n",
    "        perplexity.append(lda.score(weighted_X))\n",
    "        likelihood.append(lda.perplexity(weighted_X))\n",
    "     \n",
    "    final_topic_n = 10#likelihood.index(min(likelihood)) +3\n",
    "    lda = LatentDirichletAllocation(max_iter=100,evaluate_every=5,verbose=1,\n",
    "                                        learning_method='batch',random_state=80,n_components=final_topic_n)\n",
    "    theta = lda.fit_transform(weighted_X);\n",
    "    print(\"Log Likelihood: \", lda.score(weighted_X))\n",
    "    print(\"Perplexity: \", lda.perplexity(weighted_X))\n",
    "        \n",
    "    show_topics(lda,ivoc)\n",
    "    #print_top_words(lda, list(vocab), n_top_words=10)  \n",
    "\n",
    "    # document(chapter) - topic distribution identification \n",
    "    data_vectorized =weighted_X\n",
    "    lda_output = theta\n",
    "    topicnames = [\"Topic\" + str(i+1) for i in range(final_topic_n)]\n",
    "    docnames = [\"Chapter\" + str(i+1) for i in range(len(files))]\n",
    "    df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "    dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "    df_document_topic['Dominant topic'] = dominant_topic+1\n",
    "    \n",
    "    df_document_topic.to_html('Harry_potter_subplot.html') #this is good \n",
    "    #Counter(df_document_topic['Dominant topic'])\n",
    "    # word - topic distribution identification \n",
    "    #word distribution over topic table \n",
    "    topic_words = dict()\n",
    "    word_weights =dict()\n",
    "    n_top_words = 100\n",
    "    vocab = list(vocab)\n",
    "    lda.components = lda.components_ /lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic, comp in enumerate(lda.components_):\n",
    "       \n",
    "        word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    \n",
    "        # store the words most relevant to the topic\n",
    "        topic_words[topic] = [ivoc[i] for i in word_idx]\n",
    "        comp = comp.tolist()\n",
    "        comp.sort(reverse = True)\n",
    "        word_weights[topic] = np.round(comp,2).tolist()\n",
    "        #topic[]\n",
    "        \n",
    "    wordmap={}\n",
    "    for k in range(final_topic_n):\n",
    "        keys = topic_words[k]\n",
    "        values = word_weights[k]\n",
    "        temp = dict(zip(keys, values))\n",
    "        wordmap[k] = temp\n",
    "\n",
    "    \n",
    "    #column names\n",
    "    topicnames = [\"Topic\" + str(i+1) for i in range(final_topic_n)]    \n",
    "    df = pd.DataFrame.from_dict(wordmap)\n",
    "    df.columns=['Topic 1', 'Topic 2','Topic 3','Topic 4','Topic 5',\n",
    "                \n",
    "                'Topic 6','Topic 7','Topic 8','Topic 9', 'Topic 10']\n",
    "    \n",
    "    df =df.fillna(0)\n",
    "    df= df.div(df.sum(axis=1), axis=0)\n",
    "    df.to_html('Harry_potter_word.html')\n",
    "    \n",
    "\n",
    "    #generating graphs \n",
    "    frames = [] \n",
    "    sentence_frames = [] \n",
    "    for chapter in range(1, 200):\n",
    "        evidence = {}\n",
    "        d2 = {}\n",
    "        for i in range(1,11):\n",
    "            input_text = files\n",
    "            topic_number = i\n",
    "            chapternumber = chapter\n",
    "            #keys = topic_words[topic_number-1]\n",
    "            #wordTopic = dict(zip(keys, [topic_number]*len(keys)))\n",
    "            #clean_text = clean_str(input_text[chapternumber-1])    \n",
    "            \n",
    "            clean = input_text[chapternumber-1].replace('Mr.', 'Mister')\n",
    "            clean = clean.replace('Ms.', 'Miss')\n",
    "            clean = clean.replace('Mrs.', 'Miss')\n",
    "            lines = clean.split('.')\n",
    "            lines = list(filter(None, lines))\n",
    "    \n",
    "            sentence_dist=[]\n",
    "            for line in lines:\n",
    "                clean = line\n",
    "                words = clean.split()\n",
    "                value = 0 \n",
    "                for word in words:\n",
    "                    if word in wordmap[topic_number-1].keys(): \n",
    "                        value += wordmap[topic_number-1][word]\n",
    "                    else:\n",
    "                        value = value \n",
    "                try:\n",
    "                    agg_score = value/len(words)\n",
    "                except: \n",
    "                    ZeroDivisionError\n",
    "                sentence_dist.append(agg_score)   \n",
    "                \n",
    "        \n",
    "            input_text ='. '.join(lines)\n",
    "            # dictionary to store results (regardless of story lengths)\n",
    "            # Parse text\n",
    "            delim = \" \"\n",
    "            words = [s for s in input_text.split()]  # simplest tokenization method\n",
    "            merged_words = [\" \".join(w) for w in merge(words, 25)]\n",
    "            # Sample a sliding window of context\n",
    "            delim = \" \"\n",
    "            samples = [delim.join(s) for s in sample_window(merged_words, 20, 1)]\n",
    "            \n",
    "            \n",
    "            score = [] \n",
    "            for sample in samples: \n",
    "                try:\n",
    "                    sample = clean_text(sample)\n",
    "                except:\n",
    "                    sample = sample\n",
    "                pnt = 0 \n",
    "                words = sample.split()\n",
    "                for word in words:\n",
    "                    if word in wordmap[topic_number-1].keys(): \n",
    "                        pnt += wordmap[topic_number-1][word]\n",
    "                    else:\n",
    "                        pnt = pnt \n",
    "                try:\n",
    "                    #agg_score = value/len(words)\n",
    "                    agg_score = pnt/500 \n",
    "                except: \n",
    "                    ZeroDivisionError\n",
    "                score.append(agg_score)\n",
    "            \n",
    "            \n",
    "            d2['samples'] = samples\n",
    "            # Score sentiment using indico API\n",
    "            print(\"\\n  Submitting %i samples...\" % (len(samples)))\n",
    "            d2['Topic. '+str(i)] = score\n",
    "            print(\"  ...done!\")\n",
    "            evidence[i] = dict(zip(lines, sentence_dist))\n",
    "            \n",
    "            sentence_frames.append(evidence)\n",
    "        df2 = pd.DataFrame()\n",
    "        for k,v in d2.items():\n",
    "            df2[k] = pd.Series(v)  # keys -> columns, values -> rows\n",
    "        frames.append(df2)\n",
    "    \n",
    "\n",
    "    #main()\n",
    "    #frames = graph(wordmap = wordmap, files=files, topic_words = topic_words)\n",
    "    for i, j in zip(frames, range(1, 200)):\n",
    "        i['chapter'] = j \n",
    "        \n",
    "    names = ['chp '+str(i) for i in range(1, 200)]\n",
    "    subtexts = [len(i) for i in frames]\n",
    "    frames_df = pd.DataFrame(list(zip(names, subtexts)))\n",
    "    \n",
    "    # frames refer to the subtexts in each chapter (199 chapters)\n",
    "    frames_concat = pd.concat(frames) # a total of 36,412 passages \n",
    "    frames_concat.index = list(range(len(frames_concat)))\n",
    "    frames_concat['length'] = frames_concat.samples.apply(nltk.word_tokenize)\n",
    "    frames_concat['length'] = frames_concat.length.apply(len)\n",
    "    frames_concat = frames_concat[frames_concat.length >450]\n",
    "    frames_concat['total'] = frames_concat.drop(columns=['samples', 'chapter', 'length']).sum(axis=1)\n",
    "    final_frames = frames_concat.iloc[:,1:].div(frames_concat.total, axis=0)\n",
    "    final_frames['samples'] = frames_concat.samples\n",
    "    final_frames['chapter'] = frames_concat.chapter\n",
    "    final_frames.drop(columns=['samples', 'chapter', 'total']).plot(figsize = (20,8))\n",
    "    new_frames = final_frames.dropna()\n",
    "    new_frames.index = range(0, len(new_frames))\n",
    "    new_frames.drop(columns=['samples', 'chapter', 'total']).plot(figsize = (20,8))\n",
    "    #final_frames.to_excel('subtext_candidates.xlsx')              \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
