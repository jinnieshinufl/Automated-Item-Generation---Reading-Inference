{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep  1 13:59:36 2022\n",
    "\n",
    "@author: jinnie shin \n",
    "\"\"\"\n",
    "\n",
    "import nltk \n",
    "import json \n",
    "import string \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import get_text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def word_sentiment(files):\n",
    "    T = ('').join(files)\n",
    "    T = T.lower()\n",
    "    test_subset = nltk.word_tokenize(T)\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pos_word_list=[]\n",
    "    neu_word_list=[]\n",
    "    neg_word_list=[]\n",
    "    \n",
    "    for word in test_subset:\n",
    "        if (sid.polarity_scores(word)['compound']) >= 0.4:\n",
    "            pos_word_list.append(word)\n",
    "        elif (sid.polarity_scores(word)['compound']) <= -0.4:\n",
    "            neg_word_list.append(word)\n",
    "        else:\n",
    "            neu_word_list.append(word)    \n",
    "    \n",
    "    return neu_word_list\n",
    "\n",
    "def sentiment_weights(files):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    chapters = files \n",
    "    chapter_sents = [nltk.sent_tokenize(i) for i in chapters]\n",
    "    chapter_scores = []\n",
    "    for i in chapter_sents:\n",
    "        temp = [] \n",
    "        for j in i:\n",
    "            temp_score = np.abs(analyser.polarity_scores(j)['compound'])\n",
    "            temp.append(temp_score)\n",
    "        chapter_scores.append(np.mean(temp))\n",
    "        #chapter_scores.append(temp)\n",
    "        \n",
    "    neu_word_list = word_sentiment(files)\n",
    "    \n",
    "    # word-count vectors:\n",
    "    sentiment_wgt_dict = []\n",
    "    for chapter, chapter_score in zip(chapters, chapter_scores):\n",
    "        vect = CountVectorizer(lowercase=True)\n",
    "        X = vect.fit_transform(nltk.sent_tokenize(chapter))\n",
    "        matrix = pd.DataFrame(X.A, columns=vect.get_feature_names())\n",
    "        matrix['sentiment'] = chapter_score\n",
    "        for i in matrix.columns:\n",
    "            if i!='sentiment':\n",
    "                matrix[i] = matrix[i] * matrix.sentiment\n",
    "            else:\n",
    "                matrix[i] = matrix[i]\n",
    "                \n",
    "        sentiment_wgt = matrix.drop(columns=['sentiment']).sum().to_dict()\n",
    "        for i in sentiment_wgt.keys():\n",
    "            if i in neu_word_list:\n",
    "                sentiment_wgt[i] = 0 \n",
    "            else:\n",
    "                sentiment_wgt = sentiment_wgt\n",
    "            \n",
    "        sentiment_wgt_dict.append(sentiment_wgt) \n",
    "        \n",
    "    return sentiment_wgt_dict\n",
    "    \n",
    "\n",
    "def lemmatized_words(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    analyzer = CountVectorizer(lowercase=True,max_df=0.65,min_df=0.1,stop_words=stop_words).build_analyzer()\n",
    "    return (lemmatizer.lemmatize(w) for w in analyzer(doc))\n",
    "\n",
    "def weighted_vectorize(files): #Countvectorizer for the main model \n",
    "    \"\"\"\n",
    "    Sentiment weight matrix can be computed \n",
    "    and applied to the vectorization.  \n",
    "    \"\"\" \n",
    "    sentiment_wgt_dict = sentiment_weights(files)\n",
    "    \n",
    "    # ======= Sentiment weight matrix has to be applied here ======= #\n",
    "    \n",
    "    #with open('sentiment_wgt.json', 'r') as t:\n",
    "    #    sentiment_wgt_dict = json.load(t)\n",
    "    #additional_stopword = word_sentiment(files) \n",
    "    \n",
    "    # ===============================================================\n",
    "    \n",
    "    additional_stopword = [] #TO DO: Custom stopwords can be added. \n",
    "    \n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(additional_stopword)\n",
    "    stop_words = text.ENGLISH_STOP_WORDS#.union(additional_stopword)\n",
    "    vect = CountVectorizer(lowercase=True,max_df=0.352,min_df=0.1,stop_words=stop_words)\n",
    "    X = vect.fit_transform(files)\n",
    "    ivoc = {j:i for i,j in vect.vocabulary_.items()}\n",
    "    vocab = {i for i,j in vect.vocabulary_.items()}\n",
    "    \n",
    "    weighted_X = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        x = X.toarray()[i]\n",
    "        word_weights = sentiment_wgt_dict[i]\n",
    "        feature_names = vect.get_feature_names()\n",
    "        weights = np.ones(len(feature_names))\n",
    "        for key, value in word_weights.items():\n",
    "            try:\n",
    "                index = feature_names.index(key)\n",
    "                weights[index] = value *10\n",
    "            except: \n",
    "                weights = weights\n",
    "        weighted_X.append(np.multiply(x, weights))\n",
    "    weighted_X = np.stack(weighted_X)\n",
    "    \n",
    "    return weighted_X, ivoc, vocab\n",
    "\n",
    "# these are supposed to be considered the auxiliary LDA \n",
    "def show_topics(lda,ivoc,number_words=10,topics=range(10)):\n",
    "    for k,topic in enumerate(lda.components_):\n",
    "        if k in topics:\n",
    "            print(k+1,[str(ivoc[i]) for i in topic.argsort()[::-1][:number_words]])\n",
    "  \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_id, topic in enumerate(model.components_):\n",
    "        \n",
    "        print('\\nTopic Number.%d:' % int(topic_id + 1)) \n",
    "        print(''.join([feature_names[i] + ' ' + str(round(topic[i], 2))\n",
    "              +' | ' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "def sample_window(seq, window_size = 500, stride = 20):\n",
    "    \"\"\"\n",
    "    Generator slides a window across the input sequence \n",
    "    and returns samples; window size and stride define\n",
    "    the context window\n",
    "    \"\"\"\n",
    "    for pos in range(0, len(seq), stride):\n",
    "        yield seq[pos : pos + window_size]\n",
    "        \n",
    "def merge(seq, stride = 4):\n",
    "    \"\"\"\n",
    "    Generator strides across the input sequence, \n",
    "    combining the elements between each stride.\n",
    "    \"\"\"\n",
    "    for pos in range(0, len(seq), stride):\n",
    "        yield seq[pos : pos + stride]\n",
    "        \n",
    "def clean_text(text):\n",
    "    temp=nltk.sent_tokenize(text)\n",
    "    if temp[0].isupper():\n",
    "        temp = temp\n",
    "    else:\n",
    "        temp = temp[1:]\n",
    "        \n",
    "    if temp[-1] in string.punctuation:\n",
    "        temp = temp\n",
    "    else:\n",
    "        temp = temp[:-1]\n",
    "    \n",
    "    return ' '.join(temp)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
